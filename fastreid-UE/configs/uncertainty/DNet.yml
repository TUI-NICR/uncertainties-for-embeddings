_BASE_: ./Base-Uncertainty.yml
# This config is for the fine-tuning. 
# The pretraining consists of normal bagtricks.

MODEL:
  BACKBONE:
    # this is updated in tool script
    PRETRAIN: False # since we load a checkpoint using WEIGHTS, we do not need to load the ImageNet pretrain in the backbone
    FEAT_DIM: 2048
    FEAT_HW: (16, 8)
  
  WEIGHTS: /results_nas/ange8547/mod_arch/bagtricks
  CHECKPOINT_REMAP: [[heads.weight, crown.logit_layer.weight, 2]] # reuse logit layer # oldName, newName, number of unsqueeze(-1) to apply (squeeze if negative, squeeze() if 0)

  HEADS:
    NUM_CLASSES: 751 # used in crown

  MEAN_HEAD:
    NAME: DNet_mean_head
    POOL_TYPE: GlobalAvgPool
    DROPOUT_P: 0.5

  VAR_HEAD:
    NAME: DNet_var_head
    POOLING_SIZE: (2, 2)

  CROWN:
    NAME: DNet_crown

  FREEZE_WEIGHTS: [backbone.layer1, backbone.layer2, backbone.layer3, backbone.conv1, backbone.bn1]

  LOSSES:
    NAME: ("CrossEntropyLoss", "SampleCrossEntropyLoss", "FeatureUncertaintyLoss")

    CE:
      EPSILON: 0.1
      SCALE: 1.

    SCE:
      EPSILON: 0.1
      SCALE: 0.1

    FU: 
      SCALE: 0.001

DATALOADER:
  SAMPLER_TRAIN: NaiveIdentitySampler
  NUM_INSTANCE: 4
  NUM_WORKERS: 8

INPUT:
  SIZE_TRAIN: [ 256, 128 ]
  SIZE_TEST: [ 256, 128 ]

  FLIP:
    ENABLED: True

  REA:
    ENABLED: False
    PROB: 0.5
  
  PADDING:
    ENABLED: False

SOLVER:
  AMP:
    ENABLED: True
  OPT: Adam
  MAX_EPOCH: 50
  BASE_LR: 0.00005
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_NORM: 0.0001
  IMS_PER_BATCH: 32

  SCHED: MultiStepLR
  STEPS: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50]
  GAMMA: 0.95

  WARMUP_ITERS: 0
  CHECKPOINT_PERIOD: 2

TEST:
  METRIC: cosine
  EXTRA_END_EVAL: True
  EXTRA_END_EVAL_METRIC: [cosine, sigma_cosine, sqrt_sigma_cosine, euclidean, sigma_euclidean, sqrt_sigma_euclidean, kl_divergence, js_divergence, bhat_distance, wasserstein]
  EVAL_PERIOD: 1

# this is updated in tool script
OUTPUT_DIR: /results_nas/ange8547/mod_arch/DNet